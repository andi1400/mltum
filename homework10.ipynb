{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Machine Learning Worksheet 10\n",
      "----------------------------------------------------\n",
      "\n",
      "Programming Exercise: Linear Classification\n",
      "===========================================\n",
      "\n",
      "In this exercise you will extend an implementation of logistic regression (optimized with gradient descent) to linear classification with the \"hinge\" and \"soft-zero-one\" loss functions. You will try to understand the behavior of the different loss functions in the non- and linearly separable case, as well as analyze the impact of initialization and regularization.\n",
      "\n",
      "Set yourself a time limit for this exercise sheet and focus on understanding rather than writing perfect, extensive answers. Feel free to create helpful additional plots to accompany your answer.\n",
      "\n",
      "**Submission:**\n",
      "You may either submit a \"texed PDF\" as always (do not forget to include your code) or submit the notebook in PDF format. To print to PDF, run:\n",
      "\n",
      "    ipython nbconvert homework-10.ipynb --to latex --post PDF\n",
      "\n",
      "    \n",
      "This homework is due Friday, January 10 2014 at 23:59. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Imports and utils "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "%pylab --no-import-all inline\n",
      "from pylab import axes\n",
      "\n",
      "def addbiasdim(X): # add a first ones dimension\n",
      "    (N, D) = X.shape\n",
      "    X = np.concatenate([np.ones([N,1]), X],1)\n",
      "    return X\n",
      "\n",
      "def sigmoid(Y):\n",
      "    return 1/ (1 + np.exp(-Y))\n",
      "\n",
      "def current_model_predict(X_):\n",
      "    X_ = addbiasdim(X_)\n",
      "    y = np.dot(X_,w)\n",
      "    if loss=='logistic' or loss=='softzeroone':\n",
      "        return np.array(y >= .5, dtype=float)\n",
      "    elif loss=='hinge':\n",
      "        y[y>=0] = 1\n",
      "        y[y<0] = -1\n",
      "        return y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Code for Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logistic(X, w, z):\n",
      "    X = addbiasdim(X)\n",
      "    likelihood =  np.product(np.power(sigmoid(np.dot(X,w)),z) * np.power(1 - sigmoid(np.dot(X,w)),1-z))\n",
      "    return - np.log(likelihood)\n",
      "\n",
      "def dlogistic(X, w, z):\n",
      "    X = addbiasdim(X)\n",
      "    return - np.sum(X * (z - sigmoid(np.dot(X,w)))[:,np.newaxis], axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Regularization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def L2(w):\n",
      "    return np.inner(w[1:],w[1:]) / 2\n",
      "\n",
      "def dL2(w):\n",
      "    dw = np.zeros(w.shape)\n",
      "    dw[1:] = w[1:]\n",
      "    return dw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Hinge loss ##\n",
      "\n",
      "\n",
      "The hinge loss is given as $ \\mathcal{L}(\\mathbf{x}_i) = \\max(0, 1 - y_i  \\tilde{z}_i ), $ where $y_i = \\mathbf{x}^T \\mathbf{w}$ is the model output and $z_i$ the target variable ($w_0=b$ and $x_{i,0}=1$).\n",
      "\n",
      "<sub> Note that in this case the computation uses class labels $\\tilde{z} = 2z - 1 \\in \\{-1,1\\}$ instead of $z \\in \\{0, 1\\}$. </sub>\n",
      "\n",
      "For multiple samples $\\mathbf{X}$ and respective outputs $\\mathbf{y}$ and $\\tilde{\\mathbf{z}}$ the loss is $\\mathcal{L}(\\mathbf{X}) = \\sum_i \\mathcal{L}(\\mathbf{x}_i)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 1 ###\n",
      "Try to understand what the hinge loss does and explain it in a few of words:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 2 ###\n",
      "\n",
      "Derive the gradient of the hinge loss: (You may assume that $w_0 = b$.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "${\\mathrm{d}~\\mathcal{L}(\\mathbf{x}_i) \\over \\mathrm{d}\\mathbf{w}} = $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 3 ###\n",
      "\n",
      "Implement the hinge loss."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hinge(X, w, z):\n",
      "    return\n",
      "def dhinge(X, w, z):\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Soft zero-one loss ##\n",
      "\n",
      "The soft zero-one loss is given as $\\mathcal{L}(\\mathbf{x}_i) = \\sum\\limits_{i = 1}^{N} \\left[ \\sigma(\\beta y_i) - z_i \\right]^2, $\n",
      "\n",
      "with $y_i =  \\mathbf{x}_i^T \\mathbf{w}$ the model output and $z_i$ the target variable.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 4 ###\n",
      "\n",
      "Explain the soft zero-one loss in a few words:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 5 ###\n",
      "\n",
      "Derive the gradient ($w_0 = b$):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "${\\mathrm{d}~\\mathcal{L}(\\mathbf{x}_i)\\over \\mathrm{d}\\mathbf{w}} = ... $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 6 ###\n",
      "\n",
      "Implement the soft zero-one loss:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softzeroone(X, w, z, steepness):\n",
      "    return\n",
      "def dsoftzeroone(X, w, z, steepness):\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Configure\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loss = 'logistic'           # logistic, hinge or softzeroone\n",
      "data = 'separable'          # separable, overlapping or outliers\n",
      "max_iter = 10000            # maximum number of iterations\n",
      "stop = 1e-7                 # stop optimizing if loss is smaller than\n",
      "learning_rate = .001\n",
      "momentum = 0.\n",
      "reg = 0.\n",
      "steepness = 1.              # only for softzeroone\n",
      "\n",
      "print_freq = np.floor(max_iter / 8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Create data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.randn(500,2)\n",
      "z = np.random.choice(2,500)\n",
      "\n",
      "if data=='separable':\n",
      "    X = X + np.array([6*z-3,-6*z+3]).T\n",
      "elif data=='overlapping':\n",
      "    X = X + np.array([2*z-1,-2*z+1]).T\n",
      "elif data=='outliers':\n",
      "    X = X + np.array([4*z-2,-4*z+2]).T\n",
      "    tmp = np.random.choice(len(z),10)\n",
      "    z[tmp] = 0\n",
      "    X[tmp,0] += 8\n",
      "    X[tmp,1] -= 8\n",
      "else:\n",
      "    print 'Not an option.'\n",
      "\n",
      "if loss == 'hinge':\n",
      "    z = 2*z - 1\n",
      "    \n",
      "plt.scatter(X[z==1,0],X[z==1,1])\n",
      "plt.scatter(X[z<=0,0],X[z<=0,1],c='r')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Initialize weights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = np.zeros(3)\n",
      "w[1:] = np.random.randn(2) * 0.1\n",
      "w_init = np.copy(w)\n",
      "print 'initial weights:'\n",
      "print w\n",
      "\n",
      "ite = 0\n",
      "last_v = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Gradient descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# optimize\n",
      "startiter = ite\n",
      "print 'ite \\t\\t loss \\t\\t\\t weight magnitude'\n",
      "for ite in range(startiter, startiter+max_iter):    \n",
      "    # compute loss and gradient\n",
      "    if loss == 'logistic':\n",
      "        L = logistic(X, w ,z)\n",
      "        dL = dlogistic(X, w, z)\n",
      "    elif loss == 'hinge':\n",
      "        L = hinge(X, w, z)\n",
      "        dL = dhinge(X, w, z)\n",
      "    elif loss == 'softzeroone':\n",
      "        L = softzeroone(X, w, z, steepness) \n",
      "        dL = dsoftzeroone(X, w, z, steepness)\n",
      "        d = np.random.choice(3)\n",
      "        \n",
      "    # compute l2 regularization loss and gradient\n",
      "    L += L2(w)  * X.shape[0] * reg\n",
      "    dL += dL2(w)  * X.shape[0] * reg\n",
      "    \n",
      "    # update weight vector\n",
      "    \n",
      "    v = momentum * last_v - learning_rate * dL\n",
      "    w += v\n",
      "    last_v = v\n",
      "    \n",
      "    # output info\n",
      "    if ite%print_freq == 0:\n",
      "        print '%10i \\t %6.12f \\t %6.5f'  %(ite, L, np.sqrt(np.inner(w[1:],w[1:])))\n",
      "    \n",
      "    # stopping criterion\n",
      "    if L<stop:\n",
      "        break      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Plot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1 = np.arange(min(X[:,0]-1),max(X[:,0])+1,.1)\n",
      "\n",
      "# decision boundary\n",
      "x2_db = -( w[0] + w[1]*x1) / w[2]\n",
      "    \n",
      "if loss == 'logistic':         # saturation lines (where sigmoid = .99)\n",
      "    x2_m1 = (np.log(.01/.99) - w[0] - w[1]*x1) / w[2]\n",
      "    x2_m0 = (np.log(.99/.01) - w[0] - w[1]*x1) / w[2]\n",
      "elif loss == 'softzeroone':    # saturation lines (where sigmoid = .99)\n",
      "    x2_m1 = (np.log(.01/.99) / steepness - w[0] - w[1]*x1) / w[2]\n",
      "    x2_m0 = (np.log(.99/.01) / steepness - w[0] - w[1]*x1) / w[2]\n",
      "elif loss == 'hinge':          # margin\n",
      "    x2_m1 = (1 - w[0] - w[1]*x1) / w[2]\n",
      "    x2_m0 = (-1 - w[0] - w[1]*x1) / w[2]\n",
      "    \n",
      "plt.scatter(X[z==1,0],X[z==1,1])\n",
      "plt.scatter(X[z<=0,0],X[z<=0,1],c='r')\n",
      "plt.plot(x1,x2_db)\n",
      "plt.plot(x1,x2_m1,'gray')\n",
      "plt.plot(x1,x2_m0,'gray')\n",
      "plt.xlim(X.min(),X.max())\n",
      "plt.ylim(X.min(),X.max())\n",
      "axes().set_aspect('equal', 'box')\n",
      "plt.show()\n",
      "\n",
      "c = current_model_predict(X)==z\n",
      "print 'incorrect: %i' %sum(-c)\n",
      "\n",
      "magnitude = np.sqrt(np.inner(w[1:],w[1:]))\n",
      "direction = w[1:] / magnitude\n",
      "bias = w[0]\n",
      "\n",
      "init_magnitude = np.sqrt(np.inner(w_init[1:],w_init[1:]))\n",
      "init_direction = w_init[1:] / init_magnitude\n",
      "init_bias = w_init[0]\n",
      "print 'direction: \\t \\t \\t magnitude \\t bias \\t \\t loss \\t\\t ite'\n",
      "print '%e \\t %e \\t %e \\t %e \\t %5.5f \\t %i' %(direction[0], direction[1], magnitude, bias, L, ite) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Understanding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 7 ###\n",
      "\n",
      "Create a linearly separable dataset and perform *logistic regression*\n",
      "\n",
      "1. with very small initial weights and\n",
      "2. with large initial weights.\n",
      "\n",
      "Train multiple models for each scenario and describe what you observe: Do the models always converge to the same values? Do you understand why?\n",
      "\n",
      "\n",
      "* Use NO regularization.\n",
      "* Set momentum = 0.\n",
      "* Keep your training set constant."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 8 ###\n",
      "\n",
      "How does this change if you use L<sub>2</sub>-regularization?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 9 ###\n",
      "\n",
      "Now use the *soft zero-one* loss on a linearly separable dataset. Using no regularization and keeping other parameters constant, change the steepness parameter. What is the connection between steepness and the magnitude of the weight vector? What happens if the steepness is too large?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 10 ###\n",
      "\n",
      "Look at the *hinge* loss. Create an overlapping data set. What happens at the end of training? (This effect will be more visible if you set the learning rate to high values.) Check to see if this also happens with the logistic loss and the soft zero-one loss (do not forget to set the steepness parameter back to sensible values)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise 11 ###\n",
      "\n",
      "Work with datasets that contain outliers. Set the parameters to values that make sense to you and compare the behavior of the three loss functions. Which one handles outliers best in your opinion?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}